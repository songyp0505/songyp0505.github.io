<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>songyp0505</title>
  
  <subtitle>songyp0505&#39;Blog</subtitle>
  <link href="http://blog.176free.top/atom.xml" rel="self"/>
  
  <link href="http://blog.176free.top/"/>
  <updated>2023-12-10T08:45:02.466Z</updated>
  <id>http://blog.176free.top/</id>
  
  <author>
    <name>songyp0505</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hexo with Github Actions</title>
    <link href="http://blog.176free.top/2023/12/10/hexowithactions/"/>
    <id>http://blog.176free.top/2023/12/10/hexowithactions/</id>
    <published>2023-12-10T16:15:46.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>太不容易了</p><span id="more"></span><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p>参考网址：<br><a href="https://sotkg.link/posts/7b063777">https://sotkg.link/posts/7b063777</a></p><p>问题点：<br>    生成密钥的时候别用密码。<br>    选一个能用的action</p><h2 id="另外"><a href="#另外" class="headerlink" title="另外"></a>另外</h2><p>如何隐藏首页文章：</p><p><a href="https://blog.si-yee.com/2019/04/23/hexo-next%E4%B8%BB%E9%A2%98%E9%A6%96%E9%A1%B5%E9%9A%90%E8%97%8F%E6%8C%87%E5%AE%9A%E6%96%87%E7%AB%A0/">https://blog.si-yee.com/2019/04/23/hexo-next%E4%B8%BB%E9%A2%98%E9%A6%96%E9%A1%B5%E9%9A%90%E8%97%8F%E6%8C%87%E5%AE%9A%E6%96%87%E7%AB%A0/</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;太不容易了&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="http://blog.176free.top/categories/Hexo/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-10-09周记</title>
    <link href="http://blog.176free.top/2022/10/09/221009weeklyNote/"/>
    <id>http://blog.176free.top/2022/10/09/221009weeklyNote/</id>
    <published>2022-10-09T09:30:27.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>Done</p><span id="more"></span><p>时隔接近三个月，终于要重新鼓起勇气写周记了，这个假期发生的事情应该会铭记很久吧。</p><ol><li><p>提桶跑路的前因后果</p></li><li><p>下一步该干啥</p></li></ol><h2 id="提桶跑路"><a href="#提桶跑路" class="headerlink" title="提桶跑路"></a>提桶跑路</h2><p>首先是这一周的事情，上周把文献综述小作业交上去以后就开始考虑之前朋友说的去公司的事情。</p><p> 一开始的打算是活也不多，然后正好可以写写论文啥的，两不耽误。但是面试完出结果的时候被分到了另一个岗位，跟之前计划的差距很大。之前打算去的是“运营实习”，会有一个月的培训期，然后培训期之后就是店铺运营，但是岗位分配到了“运营助理”，直接上岗也没有培训期，忙的晕头转向的下班也没精力看论文了，感觉跟一开始的计划出入很大。</p><p>另一个原因就是，感觉做的东西没有什么意义。不知道是不是在学校里待久了的原因，总有一种“做一点对这个社会有意义的事情”的幻想，上班的公司是一家电商公司，我在的部门属于是公司的“现金牛业务”，主要是一家大药房，在天猫的权重也很高，公司对店铺的前期投入也很大，现在一天的净利润能有3W+，但是慢慢交接工作的时候，感觉很不对劲，甚至可以说是很难受。</p><p>首先是公司的销售产品，店铺虽然是大药房，但是销售的药品基本都属于“保健药品”，没有几款正规药，然后品牌主要是同仁堂，我一直以为这是个大牌子，没想到也是OEM，甚至在商品上架的时候药品注册号都是随便搜的同类型商家的，感觉对我冲击有点大，不过确实赚钱挺多，公司规模也大。</p><p>其次是公司的策略，都知道网店最重要的无非是销量和评价，工作了这几天我才知道这两个到底有多重要，公司有很大一件屋子专门是做评价的，大概有好几十个人，每天的主要工作就是刷评价和删差评。先说刷评价这个事，刷单网站是真实存在的，只是被电诈骗子给搞臭了名声，而且刷单网站也是一个很具有规模化的产业，买手通常有几十上百个淘宝账号，然后通过在刷单平台下单，商家随便发一个东西给买家，然后再返款给买家，买家等商品到货以后给商家评价，这个评价是商家已经写好了的，这也就是我当时的主要工作，就是从各个店铺中下载评价图和视频，然后自己写好评价内容发给刷单手，让刷单手进行评论，因为大部分商品都是保健药食，所以写了几天评论以后就有点，良心过不太去，通常这些评价需要涉及功效的描写，这部分又需要写的很夸张，就感觉很不负责任。另外就是删评价的事情，这部分工作虽然没接手，但是跟公司里的人聊天能知道，大部分的措施就是举报评论和跟买家交涉，利益换取差评。</p><p>还有就是公司的氛围，电商公司，大部分都是线上业务，所以沟通交流都是线上进行，即使我跟我的上司的电脑屏幕背对背，她没有什么特别的事情是不太会跟我说话的，都是直接发钉钉，干了几天两个人没说过多少话，都是在钉钉上的工作任务分配。感觉没有什么活力，但是上司确实也比较强，公司里盈利能力最强的电商运营，也就是上面提到的店铺的运营人员，不过我也确实没有对这份工作感受到一点热情，感觉每天像个行尸走肉。</p><p>假如我现在是一个今年刚毕业的应届毕业生，那我可能对这份工作很兴奋，首先是比较正常的作息时间：8：30-18：00，上午12：00-13：30休息一个半小时，没有需要加班的业务，下班就走；然后是还可观的工资，在临沂这个地方，试用期给我开的是4000，转正后有500的绩效；最后就是电商助理有便捷的方式可以直接学习电商运营，方便后期升岗加薪。</p><p>而现实是，我只是个在读学生，对学业和事业还是有一点自己的衡量标准的，况且我在申请这份工作时的初衷就是写论文的时候能捎带上点班，后来事与愿违，并且公司跟自己的价值观差距还是蛮大，于是在上了四天的班之后就提了离职，趁着沉没成本还没有那么大尽快找清楚自己的位置。</p><h2 id="下一步干啥？"><a href="#下一步干啥？" class="headerlink" title="下一步干啥？"></a>下一步干啥？</h2><p>上面也提到了跑路的原因，我觉得至少我还没有尽全力去做”论文“这件事，虽然有一点兴趣，但是这一点兴趣好像对于学术研究的话语权不太大，所以想说能不能在尽力做这件事以后看一看到底有没有持续做下去的可能，或者说是作为职业进行下去的可能。有一个有趣的小插曲是，周四的时候上班，早上我一般去的比较早，七点半左右可能就到了，之前是吃会饭睡会觉，但是周四早上看了篇论文，<a href="https://doi.org/10.1080/08959420.2016.1111725">Pension Reform in China</a>，看的很入迷，甚至有点享受，给我的工作生活带来了不小的冲击，”我到底是喜欢读论文还是不喜欢上班？“ ”读论文为什么会给自己带来这么不一样的感受“，所以我还是想能探究这个问题的答案。</p><p>接下来首先是先把答辩的PPT做了吧，十六号答辩，至少主体的框架应该先做出来，然后再一步步的往里面添加内容，还有一点就是想问一下老师文献综述还需要再在哪些方面修改一下吗？以及答辩要求里的查重率是需要自己去网站查吗，好像也没有指定网站？不过自己交的时间确实太迟了，如果排不上的话我就先按照目前写的情况做PPT了。</p><p>然后是之前老师评语里提到过的，如何把论文跟自己的专业相匹配的问题。之前有专业课的老师也提到过前几届的学长，就是因为论文跟自己的专业内容差别太大了，以至于老师都没法给指导，最后又不得不推倒重来的例子。我想着能不能在被推倒重来之前先把这项工作解决，这方面可能还是得找论文读读，我想的是先从涉及技术经济类的文献综述来看，看看能有什么研究热点能靠近的没，慢慢调整阅读策略，希望能在下周周记写一下主要的进展。</p><p>还有就是尽量再学点东西吧，总感觉这个东西有很大的惯性，一但停下来就很难再发动，之前买到的《基本无害的计量经济学》还没有认真拜读，希望在网上能找一些相关的课程认真学一下，关于慕课这个事感觉自己也有一些偏见，学什么东西的时候总喜欢优先在Youtube或者Cousera、Edx上找，而不是在中文网站学习，这种偏见好像缺少一些文化自信，不知道什么时候能改善。另外上海社科院的结业证书发下来了，真的很精美，还有整个课程期间的会议记录，老师们做的都特别用心。</p><p>最后是小论文的事情，在还没有其他头绪的时候我还是希望能把之前跟提过的小论文给完成，毕竟也做了不少努力，最近也没啥要紧事了，还是希望这段时间能把精力放在论文上。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Done&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-07-17周记</title>
    <link href="http://blog.176free.top/2022/07/17/220717weeklyNote/"/>
    <id>http://blog.176free.top/2022/07/17/220717weeklyNote/</id>
    <published>2022-07-17T22:13:58.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>两周没写周记了，这两周有点，孤独。</p><span id="more"></span><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>无</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>读完了《一只特立独行的猪》，心情复杂。人际关系一直是我木桶里最短的一块板，这两周发生的事情感觉很无助，朋友们说是经历的少了，我不懂了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;两周没写周记了，这两周有点，孤独。&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-07-03周记</title>
    <link href="http://blog.176free.top/2022/06/30/220703weeklyNote/"/>
    <id>http://blog.176free.top/2022/06/30/220703weeklyNote/</id>
    <published>2022-06-30T20:59:50.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>这周学术活动进行得比较少。</p><span id="more"></span><h2 id="论文进展"><a href="#论文进展" class="headerlink" title="论文进展"></a>论文进展</h2><p>这周论文看的比较少，前半周集中在构建整个数据处理流程了。</p><p>整个的流程其实只有前边数据分类的时候麻烦，后面分词只需要一个步骤，但是最开始的数据分类却需要很仔细地划分。</p><blockquote><ol><li><p>筛选出需要的列：时间、id、地区、正文</p></li><li><p>对时间列进行转时间序列处理，比如把“2022-04-01 20:00”分别拆进PostMonth（2022-04）和PostDay（2022-04-01）两列，用来后面构建月度数据和日度数据并作趋势图。这里出现了个问题就是数据量太分散，时间跨度太大了，如果用Python做平滑曲线处理的话坐标轴会变得很乱，所以又加了一步，把处理好的数据表导出为csv格式然后用excel作平滑处理。</p></li><li><p>根据月度数据变化，找出热度最高的五个月份，然后分别找出这五个月里热度最高的五个话题，这里面可能有重合的话题，所以不一定会是25个话题。找出话题后先对这些话题作热度分布的水平柱状图，这里可能有点多，如果篇幅有限的话后续可以换成3X3。这一步主要是为了描述不同阶段出现的不同热度，然后再根据当时的社会事件阐述产生的原因。</p></li><li><p>对初始数据按照省份分组聚合，找出频数最大前五个省份(即发贴量最高的前五个)，分别对五个省份进行话题词分析，找出讨论热度最高的话题，分析相关原因，这一部分应该是篇幅最大的部分，因为话题词比较分散，所以每个省需要生成两个大表，总共十个大表，并且需要根据不同地区的显示状态分析原因和解决办法，所以后续这部分应该是最需要细化的，当然5个也可能有点多，后续也可以换成3个。</p></li></ol></blockquote><p>暂时就想到了路径，因为看过的论文大多也就是这几部分，还没有想好自己从什么地方创新，上述的步骤也只是对样本数据写的数据处理流程脚本，后面应该能省一些功夫。另外就是绘图方面可能需要学学R，matplotlib做的图可能有点粗糙，这个后续再调整。虽然之前课程作业里写过类似的小流程，但是比较粗糙，这次稍微细化和标准化了一些，希望在流程构建这一步尽可能严谨一点，后面能比较省心。具体的代码部分还没贴上来，因为用的是ipynb文件，后续可以直接导出成markdown或者html格式，所以就打算等整个流程构建完以后做一份详细的内容再传到这里。</p><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><h3 id="读书"><a href="#读书" class="headerlink" title="读书"></a>读书</h3><p>这周读的是《章鱼的心灵》，这本书是之前那本《物理学家的智性冒险》里提到的一本书，我查了查内容以后就直接买了，不得不说，大意了。这本书跟前几次看的哲学书都不太一样，这本书是很细致很有条理的写的，前几次看的书更像是哲学家的随笔，并没有一个整体的思想框架。但是《章鱼的心灵》这本从头到尾都是讲的意识和进化（作者是一个喜欢生物学的哲学家）。作者是想通过章鱼的进化，研究章鱼的意识和智商，进而描述生物的意识进化历史，从意识这个层面，作者提到了很多很多哲学家、生物学者以及很多哲学层面的进化理论，看的我是云里雾里的，但是至少生物学的部分看懂了。</p><p>我觉得我短时间内应该不会涉及哲学一类的书籍了，学校发低保了以后又买了几本王小波的书，还没到货，在家想看的书跟在学校想看的还不太一样。在学校买的最多的书是《财新周刊》和《中国国家地理》，财新是我最喜欢的新闻媒体，一个讲社会一个讲自然，都讲的很好。但是在家就只想安安静静地看一些“真正”的书，放空自己，不去想杂七杂八的事情。</p><h3 id="职业规划"><a href="#职业规划" class="headerlink" title="职业规划"></a>职业规划</h3><p>这周还有一个困扰自己的事情，就是职业规划。之前导员在群里发过一些学长学姐的就业去向，无非就是企业&#x2F;考公&#x2F;考博。只有第二个不用一技之长，会考试就行了。进企业和考博感觉都需要不断地学习。</p><p>企业这几年也好难挨，经济下行，大厂们裁员一个比一个狠，小厂就更不用说了。进企业的话就得抓紧找实习了，我可能比较喜欢技术岗，但是不是科班出身很多东西都不了解，所以还需要学很多东西。</p><p>再一个就是考博，目前考博好像是最稳妥的路，考博需要最重要的一个东西就是“学术能力 ”，但是我好像还没有发现我有这个能力。感觉没有挖掘学术热点的脑子，不知道该干什么，我把它叫《研一综合症》。主要是研一一年过的太快了， 这种学术压力感觉越来越大，自己圈子里的朋友进展都很快，peer pressure。另外一个就是考博的职业规划是什么呢？偏人文社科的专业是不是只能去高校，研究所之类的机构也有这类专业的学者吗？</p><p>今年以来“以后能干什么”这个问题在我脑子里住下了，时不时就会出来烦我一下，时间会给自己答案吗？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这周学术活动进行得比较少。&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo英文链接</title>
    <link href="http://blog.176free.top/2022/06/30/HexoEnglishURL/"/>
    <id>http://blog.176free.top/2022/06/30/HexoEnglishURL/</id>
    <published>2022-06-30T16:13:30.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>梳理一遍如何设置文章英文地址</p><span id="more"></span><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><ol><li><p>改变URL构成：之前是用的title，现在需要改成id</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">找到配置文件_config.yml：</span><br><span class="line"></span><br><span class="line"><span class="comment">#permalink: :year/:month/:day/:title # 这是原配置</span></span><br><span class="line">permalink: :year/:month/:day/:<span class="built_in">id</span> <span class="comment"># 替换为此新配置</span></span><br></pre></td></tr></table></figure></li><li><p>设置<strong>scaffolds&#x2F;post.md</strong>，使得每次new新文章的时候可以直接在头部添加id关键词</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line"><span class="built_in">id</span>:</span><br><span class="line"><span class="built_in">date</span>: &#123;&#123; <span class="built_in">date</span> &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">categories:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><pre><code></code></pre></li></ol><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><blockquote><ol><li><p><a href="https://hexo.io/zh-cn/docs/permalinks.html">永久链接（Permalinks） | Hexo</a></p></li><li><p><a href="https://blog.csdn.net/Likianta/article/details/78943573">Hexo\scaffolds\post.md模板中的变量详解_Likianta Me的博客-CSDN博客</a></p></li><li><p><a href="https://blog.csdn.net/Likianta/article/details/79343427">如何让你的Hexo博客网址使用全英文路径_Likianta Me的博客-CSDN博客</a></p></li></ol></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;梳理一遍如何设置文章英文地址&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Win10新装WSL版CentOS7装修措施</title>
    <link href="http://blog.176free.top/2022/06/30/Win10-First-Install-Centos7ofWSLversion/"/>
    <id>http://blog.176free.top/2022/06/30/Win10-First-Install-Centos7ofWSLversion/</id>
    <published>2022-06-30T11:27:41.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下装完centos的步骤</p><span id="more"></span><h2 id="装Python3："><a href="#装Python3：" class="headerlink" title="装Python3："></a>装Python3：</h2><p><a href="https://blog.csdn.net/qq_36622490/article/details/121031026">CentOS7 Python2和Python3共存，同时安装pip3_†徐先森®的博客-CSDN博客</a></p><h2 id="Clash："><a href="#Clash：" class="headerlink" title="Clash："></a>Clash：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ip = xx</span><br><span class="line"><span class="built_in">export</span> https_proxy=<span class="string">&quot;http://192.168.0.174:7890&quot;</span></span><br><span class="line"><span class="built_in">export</span> http_proxy=<span class="string">&quot;http://192.168.0.174:7890&quot;</span></span><br><span class="line"><span class="built_in">export</span> all_proxy=<span class="string">&quot;sock5://192.168.0.174:7890&quot;</span></span><br><span class="line"><span class="built_in">export</span> ALL_PROXY=<span class="string">&quot;sock5://192.168.0.174:7890&quot;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录一下装完centos的步骤&lt;/p&gt;</summary>
    
    
    
    <category term="服务器自虐之旅" scheme="http://blog.176free.top/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%87%AA%E8%99%90%E4%B9%8B%E6%97%85/"/>
    
    
    <category term="CentOS7" scheme="http://blog.176free.top/tags/CentOS7/"/>
    
  </entry>
  
  <entry>
    <title>2022-06-26周记</title>
    <link href="http://blog.176free.top/2022/06/26/220626weeklyNote/"/>
    <id>http://blog.176free.top/2022/06/26/220626weeklyNote/</id>
    <published>2022-06-26T18:58:44.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>好快好快好快</p><span id="more"></span><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>引言</p><p>这周整理了整理小论文的第一章，然后就转头规划数据处理这部分了。</p><p>数据的问题卡在一个很尴尬的地方，就是时间的起始点。</p><p>通常这种根据微博数据的，都是根据某个或者某些具体事件，而这些事件都有具体的开始时间和热度最高时间，并且热度都很高，也就是讨论量很高，但是老龄化这些话题，普遍是阅读量很高但是讨论量不高，我只能拉长时间跨度。本来是用的开始时间是18年1月1日，也就是《中华人民共和国老年人权益保障法》修正的当年，但是整体时间跨度太长了，18年1月1日到22年5月1日。所以我就想着缩成19年1月1日到22年6月1日，也就是疫情期间，但是数据量又比较少，总共爬了有七万来条。可能微博也限制了一些抓取操作。</p><p>下一步我觉得先把整体的数据处理捋一遍，比如每一步做什么，怎么做，把流程先写好程序，等数据范围定下来以后，爬好数据直接扔进程序里出结果了，当然新数据里边也可能涉及一些隐藏信息，也可能需要重新走一遍流程。不过大多数数据清洗操作应该都是一样的，主要就是应该深入到什么地步。</p><p>另外这两天爬数据的时候老是被禁，属实有点难受。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>上次买书的时候一次买了好几本，这周把王小波的《沉默的大多数》看完了。每次看王小波的书都很享受，总是能用最朴实的语言把他那个年代的社会现实阐述的淋漓尽致，甚至很多事情放在现在也毫不违和。看过的上一本王小波的书是《爱你就像爱生命》，是王小波和李银河的书信集，十分羡慕这种“伟大的革命友谊”，也很遗憾，还没有经历过这种“伟大革命友谊”。最初看的一本是《黄金时代》，算是中篇小说，不过还是大一的时候读的了，只记得当时看的时候很惊叹，但是书里的内容不太记得了，有机会一定要再读一次。</p><p>另一件事就是重装了一下电脑，我电脑东西太多了，重装的时间成本太高了，来来回回硬是折腾了两天才把之前的软件弄好。微软虽然发了新系统win11，但是对硬件要求很严格，我就偏偏不信邪，寒假的时候各种折腾终于装上了11，但是折磨了我一个学期，学校网速慢，重装系统更费劲了，这两天整理了一下需要重装的东西，一口气装完了，win10太清爽了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;好快好快好快&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-06-19周记</title>
    <link href="http://blog.176free.top/2022/06/19/220619weeklyNote/"/>
    <id>http://blog.176free.top/2022/06/19/220619weeklyNote/</id>
    <published>2022-06-19T20:01:55.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>这周怎么这么热，天热心也热。</p><span id="more"></span><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>这周没有怎么主动检索论文，看的论文都是之前rss订阅的核心期刊里的瀑布流论文，即按照从新到旧排列的论文。</p><p><strong>1.基于社会燃烧理论的反转新闻舆情热度生成机理研究</strong></p><p>这篇论文是情报科学期刊上的一篇论文，通过“社会燃烧理论”进行事件关联方的研究。这个名词还挺有意思的：“该理论认为自然界中的燃烧现象是物理反应与化学反应共存的，燃烧现象需要燃烧物质、助燃剂和点火温度共同作用”，而作者的贡献是对21个反转新闻案例进行分析，确定出“燃烧”的不同组成部分的内容：</p><img src="/2022/06/19/220619weeklyNote/j1p1.png" class=""><p>前半部分讲的还比较通俗，但是后半部分涉及到了定性比较方法QCA，给我整的有点懵，后来又根据参考文献顺着看了几篇：<strong>组态视角与定性比较分析方法：图书情报学实证研究的新道路（情报学报）</strong>、<strong>组态视角与定性比较分析（QCA）:管理学研究的一条新道路（管理科学）</strong>，中间的一些研究步骤和路径稍微有点绕，专业名词比较多，还没深入下去。</p><p><strong>2.公共卫生事件舆情的地区差异及其情感测度——以新冠肺炎疫情为例</strong></p><p>这一篇也是情报科学上的一篇，比较新。这篇文章内容跟题目一致：爬虫抓取完微博后进行地理区域划分，对不同地区情感变化进行特征分析和原因分析，最后还做了一下回归但是没有在整篇论文中占到太大比重。</p><p>这篇论文对我一个很大启发就是，没有过于执着于研究背景和现状的细化，或者说是我太细化了，老是想面面俱到，反而越写越乱，倒不如简化精炼一点。另一点是情感计算，这篇论文也很直白地讲出来了：“<strong>由于本研究重点不在于对网络舆情分析的算法研究，所以采取现有较常用的数据挖掘与分析手段</strong>”，也没有太过渲染情感计算的过程，简单叙述了一下算法选择（LDA、TF-IDF、SnoNLP等）以后就开始了数据描述和结果说明的部分，毕竟论文的重点是<strong>舆情的地区差异及其情感测度</strong>。</p><p>另外还从RSS流里看到了两篇有意思的论文，<strong>中文人文社科领域撤销论文特征分析</strong>、<strong>融合动态主题词库和改进Shark-Search算法的主题爬虫方法——以武器装备领域为例</strong>。第一篇里讲到学术不端依然是撤销论文的主要组成部分，但是我国学术界对于撤销论文的处理办法还是不太规范，比如撤稿声明不规范、学术不端追查力度不够、数据库的撤销论文管理能力欠缺等，所以学术不端的“成本”比较低，整个学术界急需改善学术不端的风气。第二篇论文讲的是爬虫的算法设计，但是算法部分比较抽象，我希望能尝试复现一下，但是Scrapy我还没怎么学懂，主要是因为，我没太接触过爬虫方面的论文，而Github上比较热的爬虫程序制作者大部分也没有学术经历，目前的爬虫大部分都是基于链接结构的，也就是从一个页面跳转到另一个页面完成所有数据的获取，所以这个论文提到的算法还比较吸引注意力。</p><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>这周突然收到了机械工业出版社送的书，是《被讨厌的勇气》，我都快忘了这茬事了，是之前沈大图书馆组织的知识问答活动。上周读的《物理学家的智性冒险》也是被送的，不过不是出版社是亚马逊中国。这两本书都算是有一点哲学类的感觉，也算是我第一次拜读哲学类的书籍，之前一直很排斥这类书籍的原因是总感觉哲学是一门远离普通人的学科，但是这两本书都狠狠的驳回了我这个观点，一口气通读完以后就会发现：原来哲学也可以讲的这么通透、这么朴素。</p><p>另外就是浇了好几天的玉米地，感觉一年比一年热，一年比一年不正常，去年这个时候地里快涝死了，今年地里旱的，到处裂的口子，四亩地浇了好久。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这周怎么这么热，天热心也热。&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-06-12周记</title>
    <link href="http://blog.176free.top/2022/06/10/220612weeklyNote/"/>
    <id>http://blog.176free.top/2022/06/10/220612weeklyNote/</id>
    <published>2022-06-10T14:18:29.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>上次的周记已经是三周以前了。</p><span id="more"></span><h2 id="过去"><a href="#过去" class="headerlink" title="过去"></a>过去</h2><p>这几周过的也是很快，头一周是考试周，时间紧任务重，忙手忙脚的，最后一天甚至还记错了考试时间。第二周周五回的我姐家，居家隔离三天，终于腾出空看书了，把乌合之众剩下的部分好好看了看，乌合之众这本书太重了，我觉得我以后可能还会读好几次。第三周回家以后，有点迷，一天天过的没什么感觉，就把之前那本《物理学家的智性冒险》看完了，写的太棒了，虽然都是作者的随笔 ，但是读起来每一篇都很有感触，对于这个世界，同时也对自己的反思。另外还给组里的朋友们写了一份自己备份文件的方法，因为有好几个同学最近硬盘出问题了，写的好多东西都没有了。<a href="https://blog.176free.top/2022/06/10/%E6%96%87%E4%BB%B6%E5%90%8C%E6%AD%A5%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/">文件同步常用方法 | songyp0505</a></p><h2 id="现在"><a href="#现在" class="headerlink" title="现在"></a>现在</h2><p>这周也在努力的调整状态了，除了看书，每天也会看几篇论文，回顾一下小论文断了的地方，翻了翻之前组会记过的东西，对自己小论文的结构产生了很大的质疑，暂时还没有修改好。另外感觉重点要看的论文应该从“理论层面阐述养老改善”转向”舆论与政策之间的实际关系“了，了解一下主流文献中的数据处理过程、论述过程、以及数据与理论结合的过程。当然也只是侧重点变一下，毕竟引言部分还没写完，加上还有文献综述部分，所以两个方面的论文都很重要。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;上次的周记已经是三周以前了。&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>文件同步常用方法</title>
    <link href="http://blog.176free.top/2022/06/10/fileSynchronization/"/>
    <id>http://blog.176free.top/2022/06/10/fileSynchronization/</id>
    <published>2022-06-10T14:17:50.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>数据无价，记录一下自己用的数据备份方法，有点蠢但是比较省心</p><span id="more"></span><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>硬盘由于是物理介质，且是个人产品，有较大概率会出现损坏，而且由于硬盘的存取特性，当硬盘出现部分损坏时，整体数据都可能受到影响。商业网盘出于数据安全性的目的，通常会将用户数据 以分布式存放在全国（全球）各地的服务器中，即使单个服务器出现物理损坏，用户数据也不会受到影响。</p><h2 id="百度云盘"><a href="#百度云盘" class="headerlink" title="百度云盘"></a>百度云盘</h2><p>百度网盘有比较方便的工具可以直接进行文件夹备份，在“工具”下的“文件夹备份”中可以直接设置，功能处在限免阶段可以直接使用，不限网速但是限制传输量，每月10G。</p><img src="/2022/06/10/fileSynchronization/baidu.png" class=""><h2 id="OneDrive"><a href="#OneDrive" class="headerlink" title="OneDrive"></a>OneDrive</h2><p>OneDrive是微软旗下的个人存储空间，虽然功能比较局限，但是简单的上传下载也可以满足大部分的需求。</p><img src="/2022/06/10/fileSynchronization/onedrive.png" class=""><p>Onedrive自带文件同步功能，但是只能同步“桌面”、“文档”、“图片”三个文件夹</p><img src="/2022/06/10/fileSynchronization/OneDriveSync.png" class=""><p>如果需要同步其他文件夹的话，就需要通过文件夹链接的方式，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mklink /j <span class="string">&quot;F:\宋懿朋的OneDrive\OneDrive - szp365\大学&quot;</span> <span class="string">&quot;F:\大学&quot;</span></span><br></pre></td></tr></table></figure><blockquote><ol><li><p>首先要有一个OneDrive账号，也就是office账号，这个账号可以是教育邮箱（即edu.cn或edu.xxxxx结尾的电子邮箱），也可以是office365账号，有教育邮箱的话可以直接用教育邮箱去 <a href="https://www.microsoft.com/zh-cn/microsoft-365/"><strong>Microsoft 365官网</strong></a> 点击教育版注册即可，office365账号可以购买个人版、购买家庭版合租、或者直接去淘宝买账号</p></li><li><p>登陆好OneDrive账号，设置好OneDrive目录，一定不要设置在C盘！</p></li><li><p>进入OneDrive目录，例如我的是 <strong>F:\宋懿朋的OneDrive\OneDrive - szp365</strong>  </p></li><li><p>找到想备份的目录，例如我的是<strong>F:\大学</strong></p></li><li><p>通过mklink -j 建立<strong>软链接</strong>，软链接通俗来讲就是，左边的文件夹可以直接对右边的文件夹进行增删改查，但是不会额外占用硬盘空间。使用方法是：在开始菜单里找到<strong>命令提示符</strong>或<strong>CMD</strong>，然后右键选择<strong>以管理员身份运行</strong>，进入CMD后输入类似于例子中的命令即可</p></li><li><p>这样做的原理是，让OneDrive误以为OneDrive文件夹中增加了很多新文件，然后就会进行同步，实际上文件还在之前的文件夹里。</p></li></ol></blockquote><h2 id="阿里云盘"><a href="#阿里云盘" class="headerlink" title="阿里云盘"></a>阿里云盘</h2><p>阿里云盘是阿里巴巴旗下的网盘，不限速的特点吸引了很多用户，但是文件夹备份功能是会员专属，开一个网盘会员对于使用频率不高的用户来说性价比不高。WebDav可以解决这个问题，原理就是将网盘以本地磁盘的方式挂载到本地，这样就会在<strong>我的电脑</strong>显示出一个新的网盘：</p><img src="/2022/06/10/fileSynchronization/MyPC.png" class=""><p>打开以后跟本地文件夹视觉上没有什么差异</p><img src="/2022/06/10/fileSynchronization/Ali.png" class=""><p>唯一的差异就是读写速度，因为这种方式挂载的网盘，读写速度完全取决于你的网速。具体的设置方法可以参考：<a href="https://www.appinn.com/clouddrive/">CloudDrive -阿里云盘挂载为本地电脑硬盘-小众软件</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;数据无价，记录一下自己用的数据备份方法，有点蠢但是比较省心&lt;/p&gt;</summary>
    
    
    
    <category term="随笔" scheme="http://blog.176free.top/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>网站规划</title>
    <link href="http://blog.176free.top/2022/06/10/WebSitePlan/"/>
    <id>http://blog.176free.top/2022/06/10/WebSitePlan/</id>
    <published>2022-06-10T14:17:29.000Z</published>
    <updated>2023-12-10T08:45:02.470Z</updated>
    
    <content type="html"><![CDATA[<p>准备一下预谋已久的小网站，顺便深入了解一下写一个小网站需要的东西</p><blockquote><ol><li><p>显示微博话题的创建者和贡献率最高的账号</p></li><li><p>通过已有的微博热搜历史库抓取数据</p></li><li><p>根据微博热搜链接获取词条的详细数据</p></li><li><p>通过脚本将“微博话题”、“话题链接”、“话题创建者”、“话题贡献率最高用户”、“话题排名”整合到一个文件中</p></li><li><p>每天生成一个文件</p></li><li><p>网站读取上述文件后进行数据展示</p></li></ol></blockquote><span id="more"></span><h2 id="网站内容"><a href="#网站内容" class="headerlink" title="网站内容"></a>网站内容</h2><p>始作俑者</p><h2 id="网站所需工具"><a href="#网站所需工具" class="headerlink" title="网站所需工具"></a>网站所需工具</h2><p>目前了解到的工具是：Nginx、uWGSI两个</p><p>另外可能还需要用到bootstrap</p><blockquote><p>看看在招的网络工程师都需要什么工作能力，顺带学完用用。</p></blockquote><h2 id="网站进度"><a href="#网站进度" class="headerlink" title="网站进度"></a>网站进度</h2><blockquote><p>2022-06-10：计划</p><p>2022-06-20：数据获取流程构建</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;准备一下预谋已久的小网站，顺便深入了解一下写一个小网站需要的东西&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;显示微博话题的创建者和贡献率最高的账号&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通过已有的微博热搜历史库抓取数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据微博热搜链接获取词条的详细数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;通过脚本将“微博话题”、“话题链接”、“话题创建者”、“话题贡献率最高用户”、“话题排名”整合到一个文件中&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;每天生成一个文件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;网站读取上述文件后进行数据展示&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Web" scheme="http://blog.176free.top/categories/Web/"/>
    
    
    <category term="建站" scheme="http://blog.176free.top/tags/%E5%BB%BA%E7%AB%99/"/>
    
  </entry>
  
  <entry>
    <title>2022-05-15周记</title>
    <link href="http://blog.176free.top/2022/05/15/220515weeklyNote/"/>
    <id>http://blog.176free.top/2022/05/15/220515weeklyNote/</id>
    <published>2022-05-15T22:28:51.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>摘要</p><span id="more"></span><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>这周小论文卡在了几个地方，一个是数据获取，一个是框架，一个是数据处理。</p><p>在一开始的引言中我想阐述一下养老资源的现状，当然大部分肯定是阐述一下不足，因为很多论文都提到了养老资源不足的问题，不过提到具体数据的比较少。后来读到一篇提到养老床位这个问题，论文是14年的数据，提到按照9073的模式我国养老床位缺口高达25%，用的是中国民政年鉴的数据。于是我去查了查发现这个年鉴最新的数据是17年的，但是咱学校没买这个数据库，找了一大圈也没有找到源数据，买账号好像也不是很值，后来找了好几个外校的同学，终于有个上大的同学能下载，但是17年的数据跟人口数据计算后发现床位数够了！很难受，但是后来想了想可以把这个数据放在后面，积极的那部分，说一下养老资源的蓬勃发展也行，也算没白折腾。</p><p>真正敲键盘开始写才发现其实写小论文远没有自己认为的那么简单，比如想简单阐述一下养老形式的严峻，总感觉直接复制别人的不太合适，于是只能自己一遍遍的斟酌用词。虽然列好了大体框架，但是框架也需要不断细化和修改，每一小部分该些哪些方面，每个小部分该怎么衔接，都是需要重视的细节。</p><p>另外在论文阅读方面，我一开始的路线是看一些综述类的文章，跟踪一下引用文献，然后具体了解一下大部分人的研究路线和研究细节。接着确定自己的框架然后填充内容，问题就出在填充自己框架的内容。看其他人综述中写的挺好，但是引用文献中的特征其实没有那么明显，所以自己在检索比较具体的论文得时候就会很吃力，主题词框选得太多结果就很少，稍微范围大了就会爆出上千篇。如果反过来，先通过阅读大量的文献再写作，又会因为没有具体的分类和框架而让阅读记录变得很难控制，比如一篇论文读完以后做完笔记，之后还得挨个翻笔记查论文，因为从我感觉，我对论文题目不是很敏感，就是我会记得有一篇论文提到“养老床位不足”这个问题，但是却不会记得他的题目，所以还得通过笔记和历史记录等等重新翻一遍。以后对于论文的记录想想办法看看能不能有更高效地方式。</p><p>现在调整了一下，大概就是在阅读相关文献的过程中不断调整自己的整个框架，然后再去阅读文献，再总结细化并写到自己的论文里，尽量把两者结合起来。</p><p>还有一点就是参考文献会不会太多了，我写研究背景的时候因为需要从老龄化现状、养老资源现状、国家政策、跟网络信息结合等方面阐述，写到这就已经引用了十篇参考文献了，后面在数据分析后应该还会更多，我在想会不会参考文献会超量了。</p><p>另外因为老师之前提到要提出问题、假设，以及解决问题，这方面因为需要很具体，而且这个题材的论文，假设地提出好像有些模糊，所以还没有想到该怎么写，而且这个部分是不是应该靠后一点写？我看大部分论文的假设和最后的结果差别不会很大，如果一开始就提出一个预估的假设，后面做出的结果如果和假设相反会不会不太好。</p><p>所以我想着先把数据先清理一遍，对整体数据有个大概的把握，了解整个数据结果的走势再确定假设和问题之类的会不会更好一些。</p><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p>后八周就只有四门课了，项目管理、绩效管理、英语、投资学，但是前八周还有几门需要考试，好多老师已经准备提前考试以应对提前放假的可能了，所以很多期末考试也得提前准备了。</p><p>这学期的英语课是听说课，虽然一直是线上上课，但是感觉收获还是很大，因为每节课老师都会跟不同小组开视频对话，感觉自己口语好像变得稍微不那么Chienglish了，也稍微有点开口说英语的自信了，但是还是会经常卡壳，可能跟自己背单词的模式有关，背单词的时候不太开口。不过英语老师的上课模式会让人很有开口说话的欲望，这学期也学到很多英语知识，收获很大。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>准备转变一下信息获取方式，给吃灰一个学期的Kindle充上了电，网络信息太乱了，还是回归书吧，看书至少能冷静一些。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;摘要&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-05-08周记</title>
    <link href="http://blog.176free.top/2022/05/08/220518weeklyNote/"/>
    <id>http://blog.176free.top/2022/05/08/220518weeklyNote/</id>
    <published>2022-05-08T22:13:58.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>这周好烦啊！！！！！！！！！！！！！！！！！！！！！1</p><span id="more"></span><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p>老师在周二发推文的时候，我就基本锁定了老龄化和多层次养老的话题，因为其他的好像都是财会、政治、工业以及企业方面的。然后在检索论文的时候，发现能跟之前的论文挂一下钩。然后前半周其实是在爬虫方面进行了调试，我想着先看看能获取到多少数据。后来在梳理文献的时候根据各种不同的思路，想着跟数据结合一下就有了后来发给老师的框架体系。</p><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p>这周导员开会的时候说可能会提前放假然后在家里考试，时间又紧起来了，提前放假肯定要提前结课，提前结课的话整个学习流程就缩短了。下半个学期的压力还是挺大的。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>不过幸好，终于有能写的小论文了，感觉像是如释重负。之前看论文就像漫无目的的冲浪，从一篇论文沿着参考文献跳到另一篇论文，最后发现看过的文献像是大树盘根错节一样。至少现在能先从一个节点出发向上进行了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这周好烦啊！！！！！！！！！！！！！！！！！！！！！1&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-05-01周记</title>
    <link href="http://blog.176free.top/2022/04/27/220501weeklyNote/"/>
    <id>http://blog.176free.top/2022/04/27/220501weeklyNote/</id>
    <published>2022-04-27T20:35:37.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<hr><p>摘要</p><hr><span id="more"></span><h2 id="周记批改问题"><a href="#周记批改问题" class="headerlink" title="周记批改问题"></a>周记批改问题</h2><h3 id="中国舆情监控部门之间的关系"><a href="#中国舆情监控部门之间的关系" class="headerlink" title="中国舆情监控部门之间的关系"></a>中国舆情监控部门之间的关系</h3><p><strong>原文：</strong><br>中国的网络舆情监控机构遍地开花，各级别政府、企事业单位及部分私营公司的舆情监控部门相互隔离，互不统属，且自扫门前雪，各部门之间没有形成有效的联络通道<br><strong>老师：</strong><br>这种说法是不是太绝对了？完全是这样吗？</p><p>这个问题我又顺着这篇文献找了找参考文献和引用文献等等，首先对于舆情监管部门的相关研究，几乎全是硕博士论文，比如对具体地市的部门研究（本溪、普兰店等），还有对具体行业分类的舆论监管研究（食品安全、旅游、农产品、银行、医疗等）。其次对于具体地市的部门，跟上面这篇论文的描述是一样的，不过也都是对单独的地市进行的研究；其他综合性论述的文章里得出这个结论时也没有给出直接证据。</p><h3 id="https-doi-org-10-1155-2021-7647718"><a href="#https-doi-org-10-1155-2021-7647718" class="headerlink" title="https://doi.org/10.1155/2021/7647718"></a><a href="https://doi.org/10.1155/2021/7647718">https://doi.org/10.1155/2021/7647718</a></h3><h4 id="模型假设H5："><a href="#模型假设H5：" class="headerlink" title="模型假设H5："></a>模型假设H5：</h4><p><strong>原文：</strong></p><p>信息效用质量与用户信息采纳行为正相关</p><p><strong>老师：</strong></p><p>这个信息效用质量如何定义？是否用效用函数？用什么类型的效用函数？</p><p>原文中指出信息效用质量是指公众对事件信息能否满足自身需求的评价，以及信息接收者对信息价值的体验和感知。具体衡量指标包括有用性（信息内容与公众期望的差距）、及时性和相关性（信息与公众使用目的相关）。然后查到了引用的文献</p><p>[1]冯缨,张瑞云.基于用户体验的微博信息质量评估研究[J].图书馆学研究,2014(09):62-67+101.DOI:10.15941&#x2F;j.cnki.issn1001-0424.2014.09.004.</p><p>[3]李晶,漆贤军,陈明红.信息质量感知对信息获取与信息采纳的影响研究[J].情报科学,2015,33(03):123-129.DOI:10.13833&#x2F;j.cnki.is.2015.03.024.</p><p>[4]厉钟灵. 微博用户转发意愿研究——基于感知信息质量视角[D]. 浙江:浙江大学,2012.</p><p>虽然查到了，但是具体的模型还没有看懂，尤其是最后一个硕士论文，用了很多模型和变量检验方法，大体略了一下大概涉及模糊综合评价体系、因子分析、KMO测度，而且暂时也没有发现效用函数，都是用的问卷调查，然后参考国外期刊进行问卷设计，设置体现信息效用的选项，然后根据问卷数据分析得出信息效用影响用户行为的结论。</p><h4 id="舆情事件的调节作用"><a href="#舆情事件的调节作用" class="headerlink" title="舆情事件的调节作用"></a>舆情事件的调节作用</h4><p><strong>原文：</strong></p><p>舆情事件类型的调节作用：目前普遍缺乏关于舆情事件类型对用户信息采纳影响的研究，提出探索性问题：舆情事件的类型（是否为突发公共事件）对舆情信息的采纳是否有影响？</p><p><strong>老师：</strong></p><p>是否有分类？为什么没有这方面的研究呢？</p><p>以舆情分类进行模糊搜索之后，大部分的文献是根据某一类话题进行分类讨论，比如食品安全、电商评论、大学生舆情、新闻舆情等，而且研究的内容并不是以用户信息采纳为因变量，大部分是以舆情为主体进行文本分析。</p><p>具体原因可能是这类研究相比于以舆情为主体进行文本分析更加复杂，不仅需要确定用户采纳行为，还需要对舆情中的文本或其他数据进行更加细致的分析。关于舆情的研究有点两头重，一类是基于信息技术对于舆情进行模型优化或创新，计算机领域的文章偏多；一类是对于舆情进行分析，研究舆情中的发展趋势或者情感变化，对于上述这类舆情事件类型对于用户的行为的影响研究比较少。</p><h4 id="论文筛选的过程："><a href="#论文筛选的过程：" class="headerlink" title="论文筛选的过程："></a>论文筛选的过程：</h4><p>为什么绝大多数论文被舍弃了？</p><p>(1)如果研究涉及用户在舆论传播中的信息采用行为的实证研究，则选择研究</p><p>(2)为确保每项研究的独立性，如果在不同文章、会议论文和学位论文中报告的两项或多项研究基于同一个数据集，则将它们视为一项研究，仅选择一篇文章</p><p>(3)如果用户的信息采用行为是研究的因变量，则选择研究</p><p>(4)如果研究报告了影响因素和样本量之间的相关性或相关系数和p值或<em>t</em>值和样本量之间的相关性，则选择研究，这些可以转换为相关性</p><p>(5)排除了描述不明确和变量设​​计不合理的研究</p><hr><h3 id="DOI-10-13587-j-cnki-jieem-2018-02-012"><a href="#DOI-10-13587-j-cnki-jieem-2018-02-012" class="headerlink" title="DOI:10.13587&#x2F;j.cnki.jieem.2018.02.012."></a>DOI:10.13587&#x2F;j.cnki.jieem.2018.02.012.</h3><h4 id="无偏性"><a href="#无偏性" class="headerlink" title="无偏性"></a>无偏性</h4><p><strong>老师：</strong><br>媒体本身就是有偏性的，媒体内容是无偏性的又该如何刻画呢？</p><p>以媒体、无偏进行模糊搜索时文献较少，而搜索媒体负面报道时，发现一篇文献是人工标注的数据【吴艾凌,吕兴洋,谭慧敏.灾后自媒体负面报道偏差对潜在旅游者到访意愿的影响——以九寨沟“8·8”地震为例[J].旅游学刊,2019,34(04):40-50.】，通过情绪化表达、以偏概全等特征对媒体报道进行特征标注。</p><h3 id="同行评审"><a href="#同行评审" class="headerlink" title="同行评审"></a>同行评审</h3><p><strong>原文：</strong></p><p>也有论文讨论同行评审和专家评审的利弊。有一些论文讨论了国内的强人际关系可能并不适用于同行评审，这个差别也是挺有意思的。</p><p><strong>老师：</strong></p><p>的确。其实国外可能也存在这样的问题，学术观点和师承关系国外依然存在，那么为什么论文会这样说呢？有什么样的数据能够说明这个问题？</p><p>这篇论文没做好记录，忘了从哪看到的了，不过elsevier在20年对中国学者做了个调查，想了解他们对同行评审的看法，四分之三的人信任当前的同行评审流程（我觉得应该是外文期刊的同行评审），不过倒是没有提到关于国内同行评审的信息。</p><hr><p><strong>财务造假这个其实如果考虑甄别技术，我前几天看周记时就在想，财务造假新闻报道其实就有舆论对公众的影响。例如：把微博的评论换成财经网站“论坛”中的评论，那么对股价的变化是否有影响，是不是也有类似的作用？</strong></p><hr><p>这个问题我有刷到过一篇论文【史青春,徐露莹.负面舆情对上市公司股价波动影响的实证研究[J].中央财经大学学报,2014(10):54-62.】，用的是证券时报网“中国上市公司舆情中心”和“和讯网”这两个平台，通过舆情波动和股价波动结合，进行实证研究，用到了“事件研究法”和“正常收益模型”，作者说是广泛应用于会计、金融、经济领域的实证分析方法和模型，具体的专业名词我是看的有点懵，不过作者用的是不同公告的类型对于股价波动的影响，然后给出了一些建议。</p><h2 id="文献"><a href="#文献" class="headerlink" title="文献"></a>文献</h2><h3 id="1-孟凡思-钟寒-施水才-谢泽坤-基于SVM和CRF的三孩政策舆情省份差异分析-J-OL-数据分析与知识发现-1-18-2022-04-27"><a href="#1-孟凡思-钟寒-施水才-谢泽坤-基于SVM和CRF的三孩政策舆情省份差异分析-J-OL-数据分析与知识发现-1-18-2022-04-27" class="headerlink" title="[1]孟凡思,钟寒,施水才,谢泽坤.基于SVM和CRF的三孩政策舆情省份差异分析[J&#x2F;OL].数据分析与知识发现:1-18[2022-04-27]."></a>[1]孟凡思,钟寒,施水才,谢泽坤.基于SVM和CRF的三孩政策舆情省份差异分析[J&#x2F;OL].数据分析与知识发现:1-18[2022-04-27].</h3><p>这篇文献是《数据分析与知识发现》上新录的一篇，期刊归属是北大核心+CSSCI+CSCD，昨天刷rss订阅的时候发现的一篇文章。</p><p>这一篇文献的创新点可能是把舆情跟地区的政治、经济信息联系起来了。</p><p><strong>一、引言：</strong></p><p>介绍人口变化、国家政策变化、网民对于政策变化的反应。研究意义从公众参与公共事务和匿名信息对于公众的误导两个方面展开。</p><p><strong>二、相关工作</strong></p><ol><li><p>网络舆情研究趋势：</p><p>(1)针对已发生的舆情，研究舆情发生过程中的发生、传播、演化进程，探究意见领袖及相关节点的作用，以及对比分析不同主体行为、网络环境、平台机制对于舆情的影响。</p><p>(2)针对过去发生的舆情，建立数据模型以对未来的舆情变化进行预测</p></li><li><p>三孩政策的学术研究</p><p>(1)社会学：加大对基础设施建设的投入，提升二孩生育意愿进而提升三孩生育率</p><p>(2)医学：完善医疗体制</p><p>(3)舆情分析：研究情感态度和关注点，并对成因进行分析</p></li></ol><p><strong>三、传统方法的舆情分析：</strong></p><p>整体流程：抓取微博文本、预处理、情感分类、关键词提取、词云</p><ol><li><p>比较不同平台的热度（新闻网站、新闻app、电子报、微信、论坛、短视频、微博），发现微博热度最高</p></li><li><p>基于时间序列分析舆情热度</p><p>         这一段不不仅仅简单分析舆情的时间变化，还与其他舆情（离婚冷静期、延迟退休）进行了对比分析，选择三个政策各自发布后的第一周内的热度对比，发现三孩政策的热度明显高于其他两个；另外一个是作者还针对前五周的舆情进行了分析，发现周二和周六的热度明显高于其他天数，并指出工作对于人们参与网络讨论的影响。</p><p>        在对相关文本进行分析后，作者发现如生活压力、女性权利等话题存在大量内容，于是同步分析了女性权利的热度变化，发现女性权利的热度变化与三孩政策的热度变化趋势相似，但出现了明显的陡升陡降，文章结合文本信息认为在一定程度上存在网络水军进行话题营销。</p></li><li><p>情感分类</p><p>首先对于模型的数据选择，作者选出了10000条数据进行人工标注，然后比较了Bayes（贝叶斯是通过概率计算，涉及比较复杂的概率推导公式，我还没看懂。。但是书里说在文本分类里效率很高）、LSTM（长短期记忆网络，之前用过一次，就是在复杂的机深度学习网络中调整每个节点，让每个节点抛弃一些数据，从而避免过拟合的出现）、SVM和Xgboost（机器学习和深度学习里的大杀器，融合了各种算法，既高效又易用，难度在于如何确定模型组合和参数，我比较吃惊的是这个方法竟然没入选）四种方法的情感分类的准确率，<strong>但是没有提到模型中的特征选择</strong>。最后SVM的准确率最高，选择了SVM（SVM就是支持向量机，是一种二分类的算法，对于数据分类的标准是，以坐标系分类为例，SVM的任务就是找出一条直线，使得这条直线离所有的点都尽量远）。选完方法他又说了特征选择的方式：<strong>特征表示部分取文本的正面情感词数量、负面情感词数量、程度副词分值、感叹号数量、问号数量、否定词数量共六个特征构建特征</strong>。</p></li><li><p>关键词抽取</p><p>这一部分作者没有详细展开，简单提了一下有监督和无监督的使用率，最后选择了有监督的CRF，有监督也就是先提前标注好数据集，然后进行关键词提取。得出负面舆情里的关键词：</p><table><thead><tr><th>话题</th><th>主题</th></tr></thead><tbody><tr><td>全面二孩</td><td>看孩子劳累、二孩幸福、二孩教育、二孩家庭接纳、老人带孩子</td></tr><tr><td>三孩</td><td>性别平等、配套措施、生育权、婚嫁陋习、女性压力、生育负担</td></tr></tbody></table><p>然后提取负面舆情进行词云绘制，并指出词云中显示的关键词与现实生活的联系，比如女性权益、生育负担、基础设施建设等。</p></li></ol><p><strong>四、不同省份的舆情差异分析</strong></p><ol><li><p>不同省份舆情热度</p><p>发达地区和人口大省热度较高</p></li><li><p>不同省份关键词差异</p><p>作者在此处列了一个表，关键词提取的，但是我感觉提取出的关键词好像没有进行“清洗”，涉及很多无意义的关键词比如截图、调查、多地等。</p></li><li><p>不同省份舆情关键词的关联分析</p><p>河南：关键词排名较高的是学生、高三、学习等，所以结合河南的高考人数，发现河南存在较高的高考压力</p><p>北京：“劳动者”、“资本家”、“hr”、“躺平”等词占比较大，所以结合人口普查和统计图鉴，发现北京失业率较高</p><p>重庆：“养老”、“人口老龄化”、“老龄化”等词，结合人口普查数据发现重庆老龄化程度较高</p></li></ol><p><strong>五、结论</strong></p><ol><li>三孩政策在发布后较同类政策信息收获了更高的关注度</li><li>在三孩政策的网络讨论中，确实存在部分网络水军发布同质度较高的负面言论，其规模不大，影响有限，但绝不容忽视大意</li><li>对于具备一定影响力的意见领袖，不可否认对于海外用户由于其文化背景和生活经历的原因，其思想情感和价值观念同国内的主流文化存在意识形态上的较大差距，求同存异的同时也应当对其中的煽动性言论进行及时的监测和制约</li><li>从政策宣传角度来看，“为改善人口结构、促进社会经济发展而鼓励生娃”会引起民众较大的反感，而政策改进类，如“教育改革（双减）”、“防沉迷系统的改进推广”等政策宣传的舆情反响则普遍向好</li><li>不同省份的网民对三孩政策的关注点不同，所期望的后续配套措施也各有侧重，应予以适当关注</li></ol><p><strong>六、不足与展望</strong></p><ol><li><p>信息平台较为单一，未选择全平台数据信息进行分析。</p></li><li><p>数据均为自然语言文本，未进行语音图像信息和视频信息的OCR处理(这个难度就更大了，设备性能和OCR软件是巨大瓶颈)</p></li><li><p>数据信息选取的时间跨度较小，仅选取了政策相关信息发布后一个月的数据信息（微博信息失真率太高，不知大信息是什么时候抓取的，如果跟舆情发生时间离得比较近数据还会很全，但是研究失真率是不是没有什么意义？大家都知道会删帖，但是大家又都会用这个数据。）</p></li><li><p>下一步可以考虑参考不同区域的词频特征对其词频的成因进行基于人口学变量和区域社会文化上的分析，从而针对不同的区域制定更加适合区域三孩政策落实的相关计划<br>关于本文由于官方发布的三孩政策的相关信息量较少，并没有获取到相关政策的大众意见数据，在后续可以基于此继续进行针对某项制度的专项分析。</p></li></ol><p><strong>一个很明显的优势分析</strong></p><p>这篇文章的第二发表单位是<strong>北京拓尔思信息技术有限公司</strong>，我查了查这个公司，是专门做舆情处理的，很多舆情分析软件介绍的文章里也提到了这一个公司，这就使得这篇文章的数据可以尽可能地详细展示用户信息，开头介绍数据量的时候，提到共有92万余条数据，这个数据量是很庞大的，如果自己从微博爬，92万条，受限于设备数量和性能，得爬至少一个多星期，但是企业爬取的话，计算机性能、ip地址数量等都不是一个量级，获取的数据量也不是一个量级。</p><hr>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;p&gt;摘要&lt;/p&gt;
&lt;hr&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>基层治理爬虫及词云</title>
    <link href="http://blog.176free.top/2022/04/25/crawlerAndwordcloud/"/>
    <id>http://blog.176free.top/2022/04/25/crawlerAndwordcloud/</id>
    <published>2022-04-25T15:18:28.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>老师想通过百度资讯里的文章，做一个基层治理相关新闻的 词云图</p><span id="more"></span><h2 id="网址处理"><a href="#网址处理" class="headerlink" title="网址处理"></a>网址处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> jieba,wordcloud</span><br><span class="line"></span><br><span class="line">base_url = <span class="string">&quot;https://www.baidu.com/s?rtt=1&amp;bsst=1&amp;cl=2&amp;tn=news&amp;ie=utf-8&amp;word=%E4%B8%A4%E9%82%BB+%22%E5%9F%BA%E5%B1%82%22&amp;x_bfe_rqs=03E80&amp;x_bfe_tjscore=0.100000&amp;tngroupname=organic_news&amp;newVideo=12&amp;goods_entry_switch=1&amp;rsv_dl=news_b_pn&amp;pn=&quot;</span></span><br><span class="line">urls = [base_url+<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">91</span>,<span class="number">10</span>)]</span><br></pre></td></tr></table></figure><p>对于基层治理和两邻的相关新闻，共96条，10页，使用range函数划分数组组成不同的搜索结果页面</p><h2 id="获取所有网页的信息"><a href="#获取所有网页的信息" class="headerlink" title="获取所有网页的信息"></a>获取所有网页的信息</h2><p>因为数据量不大，所以直接以字典加列表的形式存储</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">results =[]</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    res = requests.get(url,headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&#x27;</span>&#125;)</span><br><span class="line">    soup = BeautifulSoup(res.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> soup.find_all(<span class="string">&quot;h3&quot;</span>):</span><br><span class="line">        result = &#123;</span><br><span class="line">            <span class="string">&quot;title&quot;</span>:i.a[<span class="string">&quot;aria-label&quot;</span>],</span><br><span class="line">            <span class="string">&quot;url&quot;</span>:i.a[<span class="string">&quot;href&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">        results.append(result)</span><br><span class="line">        <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><h2 id="确定每个网站的新闻数量"><a href="#确定每个网站的新闻数量" class="headerlink" title="确定每个网站的新闻数量"></a>确定每个网站的新闻数量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">websites = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> results:</span><br><span class="line">    <span class="keyword">if</span> i[<span class="string">&quot;url&quot;</span>].split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>] <span class="keyword">not</span> <span class="keyword">in</span> websites:</span><br><span class="line">        websites.append(i[<span class="string">&quot;url&quot;</span>].split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">web_count</span>(<span class="params">urls,keyword</span>):</span><br><span class="line">    m = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> urls:</span><br><span class="line">        <span class="keyword">if</span> keyword <span class="keyword">in</span> j[<span class="string">&quot;url&quot;</span>]:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> websites:</span><br><span class="line">    <span class="built_in">print</span>(i,web_count(results,i))</span><br></pre></td></tr></table></figure><p>得到排名前四的分别是：百家号，47，腾讯新闻，12，新浪新闻，6，搜狐新闻，8，共73条。</p><h2 id="为每个网站编写规则"><a href="#为每个网站编写规则" class="headerlink" title="为每个网站编写规则"></a>为每个网站编写规则</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_website_info</span>(<span class="params">url</span>):</span><br><span class="line">    result = []</span><br><span class="line">    p =[]</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        res = requests.get(url,headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&#x27;</span>&#125;)</span><br><span class="line">        soup = BeautifulSoup(res.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">if</span> url.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>] == <span class="string">&quot;baijiahao.baidu.com&quot;</span>: <span class="comment">#47</span></span><br><span class="line">        p = soup.find_all(<span class="string">&quot;p&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> url.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>] == <span class="string">&quot;new.qq.com&quot;</span>:<span class="comment">#12</span></span><br><span class="line">        p = soup.find_all(class_=<span class="string">&quot;one-p&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> url.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>] == <span class="string">&quot;finance.sina.com.cn&quot;</span>:<span class="comment">#6</span></span><br><span class="line">        res.encoding = <span class="string">&quot;utf-8&quot;</span></span><br><span class="line">        soup = BeautifulSoup(res.text,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line">        p = soup.find_all(<span class="string">&quot;div&quot;</span>,class_=<span class="string">&quot;article&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> url.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>] == <span class="string">&quot;news.sohu.com&quot;</span>:<span class="comment">#4</span></span><br><span class="line">        p = soup.find_all(<span class="string">&quot;article&quot;</span>,class_=<span class="string">&quot;article&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> url.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">2</span>] == <span class="string">&quot;www.sohu.com&quot;</span>:<span class="comment">#4</span></span><br><span class="line">        p = soup.find_all(<span class="string">&quot;article&quot;</span>,class_=<span class="string">&quot;article&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> p:</span><br><span class="line">            <span class="keyword">if</span> i.text != <span class="string">&quot;&quot;</span>:</span><br><span class="line">                result.append(i.text)</span><br><span class="line">    Aresult = <span class="string">&quot;&quot;</span>.join(result)</span><br><span class="line">    <span class="keyword">return</span> Aresult</span><br></pre></td></tr></table></figure><p>由于抓取的内容是文字信息，且所在的网站无明显反爬策略，所以只需要找出文章所在的位置即可。其中新浪网页编码有一些问题，需要重载编码。</p><p>只指定了排名靠前的网站的抓取策略，其余的网站会自动过滤并返回空信息。</p><h2 id="写入csv文件"><a href="#写入csv文件" class="headerlink" title="写入csv文件"></a>写入csv文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">countt = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;webinfo.csv&quot;</span>,<span class="string">&quot;w&quot;</span>,newline=<span class="string">&quot;&quot;</span>,encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    writer = csv.writer(f)</span><br><span class="line">    writer.writerow([<span class="string">&quot;title&quot;</span>,<span class="string">&quot;url&quot;</span>,<span class="string">&quot;content&quot;</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> results:</span><br><span class="line">        c = get_website_info(i[<span class="string">&quot;url&quot;</span>])</span><br><span class="line">        <span class="keyword">if</span> c != <span class="string">&quot;&quot;</span>:</span><br><span class="line">            content = c</span><br><span class="line">            w = &#123;</span><br><span class="line">                <span class="string">&quot;title&quot;</span>:i[<span class="string">&quot;title&quot;</span>],</span><br><span class="line">                <span class="string">&quot;url&quot;</span>:i[<span class="string">&quot;url&quot;</span>],</span><br><span class="line">                <span class="string">&quot;content&quot;</span>:c</span><br><span class="line">            &#125;</span><br><span class="line">            writer.writerow([w[<span class="string">&quot;title&quot;</span>],w[<span class="string">&quot;url&quot;</span>],w[<span class="string">&quot;content&quot;</span>]])</span><br><span class="line">            countt += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> countt % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;已完成&#123;&#125;条&quot;</span>.<span class="built_in">format</span>(countt))</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h2 id="分词处理"><a href="#分词处理" class="headerlink" title="分词处理"></a>分词处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read webinfo.csv</span></span><br><span class="line">p_list = []</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;webinfo.csv&quot;</span>,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    reader = csv.reader(f)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> reader:</span><br><span class="line">        <span class="keyword">if</span> i[<span class="number">0</span>] == <span class="string">&quot;title&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p_list.append(i[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将文章分词</span></span><br><span class="line">p_list_2 = []</span><br><span class="line">jieba.enable_paddle()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> p_list:</span><br><span class="line">    seg_list = jieba.cut(i, cut_all=<span class="literal">False</span>)<span class="comment">#精确模式，即尽可能保留名词</span></span><br><span class="line">    p_list_2.append(<span class="string">&quot; &quot;</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取停用词并删除</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;baidu_stopwords.txt&quot;</span>,<span class="string">&quot;r&quot;</span>,encoding=<span class="string">&quot;utf-8-sig&quot;</span>) <span class="keyword">as</span> stopwords:</span><br><span class="line">    stop_words = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> stopwords.readlines()]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> p_list_2:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> stop_words:</span><br><span class="line">            p_list_2.remove(i)</span><br><span class="line"></span><br><span class="line">data_1 = <span class="string">&quot;&quot;</span></span><br><span class="line">data_1 = <span class="string">&quot;&quot;</span>.join(p_list_2)</span><br></pre></td></tr></table></figure><h2 id="词云绘制"><a href="#词云绘制" class="headerlink" title="词云绘制"></a>词云绘制</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, ImageColorGenerator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 第三方库PIL是图片处理库，默认安装，如果没有，就需要自己安装</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">font = <span class="string">r&#x27;C:\Windows\Fonts\SIMLI.ttf&#x27;</span>;</span><br><span class="line">py_mask = np.array(Image.<span class="built_in">open</span>(<span class="string">&#x27;LN6.png&#x27;</span>))</span><br><span class="line"><span class="comment"># 读取颜色</span></span><br><span class="line">img_colors = ImageColorGenerator(py_mask)</span><br><span class="line"><span class="comment"># 输入wordcloud</span></span><br><span class="line">wc1 = WordCloud(mask = py_mask, font_path=font, stopwords=stop_words,background_color=<span class="string">&quot;white&quot;</span>,width=<span class="number">400</span>,height=<span class="number">400</span>).generate(data_1)</span><br><span class="line"><span class="comment"># 生成词云</span></span><br><span class="line"><span class="comment">#wc1.generate(data_1)</span></span><br><span class="line"><span class="comment"># 上色</span></span><br><span class="line">wc1.recolor(color_func=img_colors)</span><br><span class="line"><span class="comment"># 展示</span></span><br><span class="line">plt.imshow(wc1, interpolation=<span class="string">&#x27;bilinear&#x27;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line">wc1.to_file(<span class="string">&#x27;test.png&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;老师想通过百度资讯里的文章，做一个基层治理相关新闻的 词云图&lt;/p&gt;</summary>
    
    
    
    <category term="Python" scheme="http://blog.176free.top/categories/Python/"/>
    
    
    <category term="爬虫" scheme="http://blog.176free.top/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>疫情数据清洗</title>
    <link href="http://blog.176free.top/2022/04/25/pandemicDataClean/"/>
    <id>http://blog.176free.top/2022/04/25/pandemicDataClean/</id>
    <published>2022-04-25T15:18:11.000Z</published>
    <updated>2023-12-10T08:45:02.470Z</updated>
    
    <content type="html"><![CDATA[<p>因为服务业作业需要用到疫情数据，所以重新找了一些数据处理步骤</p><span id="more"></span><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><ol><li><p>Github，<a href="https://github.com/BlankerL/DXY-COVID-19-Data">DXY-COVID-19-Data</a></p></li><li><p>霍普金斯大学，<a href="https://coronavirus.jhu.edu/map.html">Covid Data</a></p></li></ol><h3 id="Github数据处理"><a href="#Github数据处理" class="headerlink" title="Github数据处理"></a>Github数据处理</h3><p><strong>导入数据并筛选国内数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment">#读取所有数据作为data_1</span></span><br><span class="line">data_1 = pd.read_csv(<span class="string">&#x27;DXYArea.csv&#x27;</span>)</span><br><span class="line"><span class="comment">#删除provinceName是中国的数据行</span></span><br><span class="line">indexNames = data_1[data_1[<span class="string">&quot;provinceName&quot;</span>]==<span class="string">&quot;中国&quot;</span>].index</span><br><span class="line"><span class="comment"># Delete these row indexes from dataFrame</span></span><br><span class="line">data_1.drop(indexNames, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#筛选countryName是中国的数据</span></span><br><span class="line">data_2 = data_1[data_1[<span class="string">&quot;countryName&quot;</span>]==<span class="string">&quot;中国&quot;</span>]</span><br><span class="line"><span class="comment">#筛选特定行</span></span><br><span class="line">save_series = [<span class="string">&quot;provinceName&quot;</span>,<span class="string">&quot;provinceEnglishName&quot;</span>,<span class="string">&quot;province_confirmedCount&quot;</span>,<span class="string">&quot;province_curedCount&quot;</span>,<span class="string">&quot;province_deadCount&quot;</span>,<span class="string">&quot;updateTime&quot;</span>]</span><br><span class="line">data_2 = data_2[save_series]</span><br><span class="line">data_2.head(<span class="number">2</span>)</span><br><span class="line"><span class="comment">#删除同一天中的相同时间节点数据</span></span><br><span class="line">data_2.drop_duplicates(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>其中批量删除行用到的操作是：先获取有指定值的行的索引数据，然后使用drop()函数drop掉指定索引的行，删除重复行使用的是drop_duplicates()函数</p><p><strong>按不同省份划分数据集：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按省划分数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_pro</span>(<span class="params">data</span>):</span><br><span class="line">    datas = []</span><br><span class="line">    province_list = data[<span class="string">&quot;provinceName&quot;</span>].unique()</span><br><span class="line">    <span class="keyword">for</span> province <span class="keyword">in</span> province_list:</span><br><span class="line">        datas.append(data[data[<span class="string">&quot;provinceName&quot;</span>]==province])</span><br><span class="line">    <span class="keyword">return</span> datas</span><br></pre></td></tr></table></figure><p>首先确定好“provinceName”列的不重复值，然后根据这些数据对数据集进行划分，并将所有省份的数据集存储在datas列表中返回</p><p><strong>处理时间数据：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对每个省份分别处理时间数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">time_process</span>(<span class="params">data</span>):</span><br><span class="line">    datas = pd.merge(data, pd.DataFrame(data[<span class="string">&#x27;updateTime&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27; &#x27;</span>,expand=<span class="literal">True</span>)), how=<span class="string">&#x27;left&#x27;</span>, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>)</span><br><span class="line">    datas = datas.groupby(<span class="number">0</span>)[<span class="number">1</span>].agg(<span class="string">&#x27;max&#x27;</span>).reset_index()</span><br><span class="line">    datas[<span class="string">&quot;updateTime&quot;</span>] = datas[<span class="number">0</span>] +<span class="string">&quot; &quot;</span>+ datas[<span class="number">1</span>]</span><br><span class="line">    datas.drop([<span class="number">0</span>,<span class="number">1</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    data = datas.merge(data,how=<span class="string">&#x27;inner&#x27;</span>,on=<span class="string">&#x27;updateTime&#x27;</span>)</span><br><span class="line">    <span class="comment">#添加每日新增数据</span></span><br><span class="line">    data[<span class="string">&quot;province_currentConfirmedCount&quot;</span>] = data[<span class="string">&quot;province_confirmedCount&quot;</span>] - data[<span class="string">&quot;province_curedCount&quot;</span>] - data[<span class="string">&quot;province_deadCount&quot;</span>]</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>这步操作的原因是，源数据中是按照市级单位抓取的数据，但是这里只需要用到省级数据，所以只需要保留每天最后一条数据中的省级数据列。这里其实有个遗留问题就是，如何确定数据正确，因为issues里提到有一些地区的数据存在错误，作者解释道可能是丁香医生工作人员在录入数据时出现错误，但是由于爬虫仅爬取数据所以未对数据进行修正。但是从后续结果看，这些错误不太影响整体结果。</p><p>如何确定每天的最后一条数据？因为时间序列的操作没整明白，就用了原始的操作，即切割字符串，根据字符串比较，选出最大的字符串后汇编成一列，然后根据挑选出的数据左联，得出每天最后一条数据的数据集，并添加日新增的数据列</p><p><strong>按省份处理数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#处理省份数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pro_process</span>(<span class="params">datas,name</span>):</span><br><span class="line">    datas_1 = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> datas:</span><br><span class="line">        i = time_process(i)</span><br><span class="line">        datas_1.append(i)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> datas_1:</span><br><span class="line">        <span class="keyword">if</span>(j[<span class="string">&quot;provinceName&quot;</span>].unique() == name):</span><br><span class="line">            res = j</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>这一步根据传入的不同省市，输出处理好的数据</p><p><strong>清洗和汇总每个省份的日新增数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#处理具体省份具体数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pro_process_1</span>(<span class="params">data</span>):</span><br><span class="line">    data = data.groupby(<span class="string">&quot;updateTime&quot;</span>)[<span class="string">&quot;province_currentConfirmedCount&quot;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line">    data_1 = times(data)</span><br><span class="line">    <span class="keyword">return</span> data_1</span><br></pre></td></tr></table></figure><p>这一步是将数据集中的数据转换成只有时间和日新增确诊的数据表，以存储为csv格式进行后续的图表绘制等操作。</p><p><strong>整体步骤</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">datas = split_pro(data_2)</span><br><span class="line">guangzhou = pro_process(datas,<span class="string">&quot;广东省&quot;</span>)</span><br><span class="line">guangzhou = pro_process_1(guangzhou)</span><br><span class="line">guangzhou.to_csv(<span class="string">&quot;guangzhou.csv&quot;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>如果需要获取全国的日新增数据，只需要对data_2进行时间处理，然后进行新增数据计算并汇总时间数据即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#处理时间数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">times</span>(<span class="params">data</span>):</span><br><span class="line">    data_1 = pd.merge(data, pd.DataFrame(data[<span class="string">&quot;updateTime&quot;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27; &#x27;</span>,expand=<span class="literal">True</span>)), how=<span class="string">&#x27;left&#x27;</span>, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>)</span><br><span class="line">    data_1 = data_1.groupby(<span class="number">0</span>)[<span class="number">1</span>].agg(<span class="string">&#x27;max&#x27;</span>).reset_index()</span><br><span class="line">    data_1[<span class="string">&quot;updateTime&quot;</span>] = data_1[<span class="number">0</span>] +<span class="string">&quot; &quot;</span>+ data_1[<span class="number">1</span>]</span><br><span class="line">    data_1.drop([<span class="number">0</span>,<span class="number">1</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#获取每天最后一条日期的数据并存储</span></span><br><span class="line">    data_1 = data_1.merge(data,how=<span class="string">&#x27;inner&#x27;</span>,on=<span class="string">&#x27;updateTime&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> data_1</span><br><span class="line"></span><br><span class="line">datas_1 = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> datas:</span><br><span class="line">    i = time_process(i)</span><br><span class="line">    datas_1.append(i)</span><br><span class="line">test = pd.concat(datas_1)</span><br><span class="line">test_1 = test.groupby(<span class="string">&quot;updateTime&quot;</span>)[<span class="string">&quot;province_currentConfirmedCount&quot;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line">test_2 = times(test_1)<span class="comment">#因为对每个省的数据汇总后会有当天时间重合的部分，所以需要去除</span></span><br><span class="line">test_3 = test_2[test_2[<span class="string">&quot;updateTime&quot;</span>].<span class="built_in">str</span>.startswith(<span class="string">&quot;2020&quot;</span>)]</span><br><span class="line"><span class="comment">#将test_3转换成csv文件并存储在本地</span></span><br><span class="line">test_3.to_csv(<span class="string">&quot;test_3.csv&quot;</span>,index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="霍普金斯大学数据处理"><a href="#霍普金斯大学数据处理" class="headerlink" title="霍普金斯大学数据处理"></a>霍普金斯大学数据处理</h3><p>这个数据集就很清晰，不用做太多工作，直接筛选出新增数据和时间戳就行了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;因为服务业作业需要用到疫情数据，所以重新找了一些数据处理步骤&lt;/p&gt;</summary>
    
    
    
    <category term="Python" scheme="http://blog.176free.top/categories/Python/"/>
    
    
    <category term="pandas" scheme="http://blog.176free.top/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>2022-04-24周记</title>
    <link href="http://blog.176free.top/2022/04/24/220424weeklyNote/"/>
    <id>http://blog.176free.top/2022/04/24/220424weeklyNote/</id>
    <published>2022-04-24T22:59:01.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>上学期结束，一些课程作业也暂时告一段落了。</p><span id="more"></span><h2 id="课业"><a href="#课业" class="headerlink" title="课业"></a>课业</h2><p><strong>上半周</strong></p><p>首先是交完了三个课程作业，之前周记里说错了，不是学术伦理，是职业道德与伦理的课程，因为商学院负责授课的老师是会计及财务管理方面的老师，所以布置了财务造假的案例作业。另外学术写作课程的作业也修改了一下重新提交了。</p><p>上半周花时间最多的是服务业课程的课程论文，第一次提交的论文内容比较仓促，主要对疫情和物流的数据进行了分析，得出了一个”确诊病例上升物流量下降，反之上升“的结论，后来老师说这个结论很显而易见，问我这个结果有什么意义吗？转头一想，确实没什么意思，这个结论就是个常识。</p><p>后来老师给了一些建议，说能不能以问卷的形式从消费者的角度切入，进行一些研究，我查了一两天的资料后感觉问卷的形式太不可控了，首先设计出高质量的问卷是一件要求特别高的事情，并且在问卷的发放过程中，我所能触及的人数很少，如何收集足够数量的问卷也是个问题。后来我就开始着手从”黑猫投诉“网站上爬快递公司们的用户投诉，因为黑猫上的数据量很大，而且用户群体也很广泛，程序写了一天，没能突破黑猫的反爬虫策略，查了很多资料后也没能解决这个问题。后来又了解到邮政总局每个月都会公布用户投诉的详情，于是最后选择了邮政总局的数据，邮政总局的数据会对用户投诉进行详细的分类，这对后面论文的书写有很大帮助，最后通过这些数据分析出随着疫情波动哪些投诉类型占比较大、变化较大等，根据不同的分析结果给出不同的改进建议。</p><p><strong>下半周</strong></p><p>下半周开始着手处理区域治理的数据，因为下半周课程比较多，所以进度相对就没有那么快。这部分的内容时间花的比较多的是CiteSpace的调图和百度上的数据爬取。CiteSpace虽然很多论文都在用，但是这个软件是没有一个正经的组织进行软件维护的，基本就是陈超美教授和他的团队不定时修复bug，大部分学者的软件都是从陈超美教授的博客上下载的，而源网站上是有很多版本的，新版本往往不稳定，旧版本又有一些不完善的地方，我换了不下五个版本才顺利运行出来，但是还是有一些问题，比如图层卡顿等问题。这也是为什么在CiteSpace上花了那么多时间的原因。</p><p>另外就是爬取百度上的数据，因为百度是一个搜索引擎，它只索引网站，获取全文还是需要到新闻源网站进行爬取，但是不同的网站的网页结构是不一样的，这就极大增加了工作量。幸好百度这几年力推“百家号”这个模块，96个结果中有36个都是百家号的内容。在一开始我是打算遍历出不同网站然后为每个网站单独写一个爬虫，但是后来发现这个工作量实在是太大了，可能得写好几天，还不如直接复制粘贴了。后来观察这些网址信息后，发现百家号+腾讯新闻+搜狐新闻+网易新闻四个网站加起来就已经有73条新闻了，这样只写四个逻辑就可以了，后来进展就快了，爬取全文、分词、删词、生成词云图。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>另外这周老师发的批改内容还没来得及一一回答，争取这周能重新回顾一下当时的场景，找找一些问题的解答。</p><p>从这一周开始课程量就没那么大了，需要正经看文献了。区域治理的相关文献是一部分，之前看过的论文希望能继续探究研究方向。旁边宿舍有师范的哥们，两年就毕业了，马上就要开题了，属实是太快了。李玉龙老师写课程论文之前就问我有研究方向吗，有的话可以直接写相关的课程论文，但是确实还没有个明确的方向，还是得深入到论文里。</p><p>之前周记里提到的总结pandas处理数据，也没来得及写，加上这周爬虫和词云的一些操作，这次汇总一下。这些都是学得快也忘得快的东西，因为每天进行的任务不会频繁涉及到这些工具，所以每次用完就会有很长时间的空档，如果不记录的话，很快就忘了，每次使用的时候都要重新去Google操作步骤，繁琐不说，这些步骤通常是很广的，还要根据情况修改，自己记录自己使用的过程会让这些用法很清晰，代码复用是一个十分提高效率的方法。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;上学期结束，一些课程作业也暂时告一段落了。&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-04-18周记</title>
    <link href="http://blog.176free.top/2022/04/17/220418weeklyNote/"/>
    <id>http://blog.176free.top/2022/04/17/220418weeklyNote/</id>
    <published>2022-04-17T21:43:14.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>这周好像全是搞论文，又是飞速过去的一周，不知道什么时候快递能解封。</p><span id="more"></span><h2 id="服务业论文"><a href="#服务业论文" class="headerlink" title="服务业论文"></a>服务业论文</h2><p>这周的大部分时间其实花在了服务业课程论文上了，我想写疫情对物流业的影响，然后选的是2020年的数据，通过国家统计局下载好快递数据后就开始找新冠数据，没想到，我竟然找了两天。</p><p>一开始是直接在Google上搜，只找到一些笼统的数字，忽略了一些小一点标签，致使没发现霍普金斯大学发布的数据，然后去GitHub上找，找到了一个很大的数据仓库，从2020年1月份开始记录的。但是我下载下来后才发现这个巨大的数据仓库十分冗杂，爬虫程序会每天多次爬取数据，精确到每个市，但是由于是不同的时间点，如果想要获取每个省的数据还需要额外的数据清洗。于是从周二到周四我一直在处理这些数据。很多pandas的内容都遗忘了，看了一些官方文档才想起来，从数据切割到筛选等。这里为什么没有用excel之类的程序是因为，数据报表太“大”了，九十多兆，一千多万条数据，excel打开会变得特别卡，毕竟是图形化处理，占用计算机资源太多了。后来改成pandas处理，效率直接就提上来了。</p><p>数据处理完成以后，发现数据的波动太大了，因为疫情是分批的，哪里爆发一次之后新增确诊病例数量就会大幅度上升，疫情缓和以后就又很低，画图的时候就跟心率好几百的心电图一样，然后对数处理以后缓和了一点。</p><p>当我再次在谷歌搜索疫情相关数据的时候，突然想到了霍普金斯大学的日增数据，然后发现霍普金斯大学的数据中心有中国整体的数据，既喜又悲，因为我光是清洗countryName是中国的数据就清洗了好久，结果发现了现成的，好在数据差别不是很大。</p><p>后来在筛选物流城市的疫情数据的时候，之前“走过的弯路”给了我很大帮助，很多重复的操作写个函数把之前的步骤放进来就好了，效率直线提升。比如筛选固定数据列、筛选每个城市每天最晚的数据、按值合并不同的数据表、按各省份划分整体数据表等等冗长的操作。</p><p>不过由于想法不多，导致论文内容也不是很丰富，不知道课程老师会不会有什么建议。</p><h2 id="财务造假分析"><a href="#财务造假分析" class="headerlink" title="财务造假分析"></a>财务造假分析</h2><p>这个课程是姜昕老师给我们上的，老师布置完作业我还特意问了问老师还有没有别的题目，老师说只有这个财务造假的了，因为不太熟悉这方面的，所以跟老师沟通了一下，不过老师说不用写的很深，有问题再沟通。</p><p>在查了一些资料后发现可以直接在“问财”网站上搜索“ 处罚原因信息披露违规 ”就可以搜索到上市公司的处罚案例，查了一些案例后发现，会计好复杂啊，眼花缭乱的名词和造假手段，以至于我在看一些处罚数额的时候都有点迷糊，不知道是什么环节的职务出现了问题，但是很神奇，比如一些董事长高管被禁入市场，那他们的股票怎么办，砸手里了吗？但是这些人造假了那么多个亿，这点小事好像难不倒他们。</p><p>此外还有学术写作的论文，老师要求比较高，对格式和字数也都有要求，于是就把之前寒假写过的论文整理了一下格式和添加了一些国外论文。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这周好像全是搞论文，又是飞速过去的一周，不知道什么时候快递能解封。&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2022-04-10周记</title>
    <link href="http://blog.176free.top/2022/04/10/220410weeklyNote/"/>
    <id>http://blog.176free.top/2022/04/10/220410weeklyNote/</id>
    <published>2022-04-10T13:34:47.000Z</published>
    <updated>2023-12-10T08:45:02.462Z</updated>
    
    <content type="html"><![CDATA[<p>还没想好摘要</p><span id="more"></span><h2 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h2><p>虽然这周周六没补课，但是还是过的飞快！眨眼间就过完了一周。大部分的课还是能听懂的，但是计量经济是个例外。可能受限于只有8周课，老师每次课都讲得比较快，而且线上上课我们之间的互动也不多，很多有疑问的地方也没有能顺畅的进行交流，结果就是马上快完了还懵懵懂懂的。老师也提过后八周可能会给大家布置一些作业，能让更好的回顾课程知识。另外其他的课程论文也该提上日程了，学术伦理课的课程论文是会计案例，这可是稍微有点难住了，跟任课老师沟通了一下，老师说不用写太深，放松了一口气。</p><h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p>这周其实大部分时间花在了一些kaggle课程上了，论文主要找了一些服务业课程里的相关论文。老师说让我们查查“共享式服务创新”的相关论文，但是越看我越觉得不对劲。共享这个词好像“变味”了。</p><p><strong>戴克清,陈万明.共享式服务创新的逻辑、形式与价值——制造业服务化转型视角[J].软科学,2020,34(09):30-36.DOI:10.13956&#x2F;j.ss.1001-8409.2020.09.05.</strong></p><p>这篇文章用的是扎根理论分析【扎根理论研究一定要有经验证据的支持。但它的主要特点不在其经验性,而在于它从经验事实中抽象出了新的概念和思想。扎根理论特别强调从资料中提升理论,认为只有通过对资料的深入分析,才能“归纳”,从下往上将资料不断地进行浓缩,形成理论框架】，对海尔、沈阳机场、陕西鼓风机集团进行了分析。通过对企业服务化举措的关键词进行分析、编码、要素提炼，通过共线分析再进一步划分特征得出结论：</p><p>共享式服务创新(SSI)是制造业企业基于共享经济模式特征,依托智能制造和平台化建设实现技术变革，通过开放化和服务化的组织管理导向实现管理变革,以共享认同为基本理念,以服务主导为基本逻辑,以价值共创为基本行为,实施制造业企业服务化转型的服务模式创新过程。</p><p><strong>贾明,阮宏飞,张喆.公司澄清能制约媒体传谣吗:基于A股上市公司的经验证据[J].管理工程学报,2018,32(02):107-118.DOI:10.13587&#x2F;j.cnki.jieem.2018.02.012.</strong></p><p>这一篇是续了上一周的相关方向，这篇文章的创新点在于从媒体报道有偏性的视角研究了公司行为对媒体报道偏差的影响。文章里边提到“<strong>总结前人的研究发现，国内大多数学者从媒体报道的无偏性视角研究了媒体在公司治理中发挥的积极作用，而忽视了媒体报道可能存在偏差的现象，且目前这一领域在许多方面还存在空白</strong>”，其实让我很不理解，媒体报道存在偏差应该是一个常识，至少在我接触移动互联网后是这么认为的，每家媒体的定位不同，利益不同，所报道的内容当然不同。—更新：引文是12年的文章，通过知网和谷歌学术关于（媒体+中立&#x2F;偏）的主题搜索，文献数量已经有很多了。</p><p>文章主要研究了澄清公告和上市公司实际控制人的行政级别对于媒体报道偏差的影响。也是通过计算不同阶段的媒体情感数据，做公告发布前后的情绪对比。但是中间对于澄清公告质量的评价，采用了很多参数。通过各参数组成的模型进行实证分析，研究表明对于负面传闻而言，上市公司发布澄清公告能够对媒体报道偏差产生治理效果，而且澄清公告的澄清质量越高，澄清之后媒体负面情绪越小。</p><h3 id="共享"><a href="#共享" class="headerlink" title="共享"></a>共享</h3><p>上文提到的共享经济的发展中，其实自从“共享经济”这个词火起来的时候我就没get到这个词的适用性。共享单车、共享充电宝这些实体经济没有一个是跟共享沾边的，明显是租赁。包括沈阳机床厂的“共享模式”，也是套了共享经济的外壳，核心还是租赁。跟地方政府共建厂房，把机床分散在各个厂房中，给当地的中小企业租赁。虽然我找了很久也没找到这个措施的实施效果（因为大多数文章只是一提成果不佳），但是在沈机推出共享模式没多久就陷入危机了。</p><p>在知乎专栏上有一篇<a href="https://www.zhihu.com/question/40998217/answer/178436689">文章</a>对于共享和租赁的区写的比较清楚：</p><img src="/2022/04/10/220410weeklyNote/1.jpg" class=""><p>一个明显的区别就是业务模式到底是C2C还是B2C。另外，是否存在平台、轻重资产、网络效应和市场趋势，二者都有明显的区别，并且从这些区别中能很直观的发现，现有的大部分共享经济企业都是租赁型企业。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="kaggle"><a href="#kaggle" class="headerlink" title="kaggle"></a>kaggle</h3><p>逛YouTube的时候刷到了个如何参加kaggle的视频，于是进kaggle学了一遍入门课程。</p><p>kaggle是一个数据分析类的网站，既有一些课程也有一些比赛。国内相似的就是阿里天池大赛。Kaggle网站里的入门课程我过了两个，一个是入门机器学习，一个是中级机器学习。有一个很有意思的地方就是，国外这种设计代码的课程网站，课程进度都设计得让人很有成就感。比如阿里天池里的新手文档，他会给你一个完整的notebook，里面有各个步骤的代码，然后在代码上面或下面叙述代码相关的用途，然后在课后练习中基本上就是把之前课程里的代码背下来写上就没错。而Kaggle里的课程里有一套完整的“养成系统”，在课程学习阶段只是讲述代码和理论之间的结合，而在课后练习中会在每一步都给予反馈，每一步都会进行的很有成就感。这个模式在之前的Sololearn网站中我也体验过，就是会让人很想继续下去。</p><p>关于kaggle的内容可能内容比较无聊，并且涉及代码的内容比较多，就没有放在这里，具体的文章放在了blog里：<a href="https://blog.176free.top/categories/Kaggle/">分类: Kaggle | songyp0505</a></p><h3 id="同行评审"><a href="#同行评审" class="headerlink" title="同行评审"></a>同行评审</h3><p>这是一个前两天突然想到的问题，因为平常在浏览很多国外期刊的时候都会看到<strong>Peer review</strong>的字样，而且在一些社交平台上也刷到过一些国外期刊（医学类）让博主进行论文评审的帖子，于是就找了找国内关于同行评审的信息，知网上的论文讨论的都是国外的同行评审，关于国内的比较少，而且以国内期刊同行评审的相关关键词进行搜索时也没有获得太多信息。国内的优秀期刊好像大部分都是专家评审，也有论文讨论同行评审和专家评审的利弊。有一些论文讨论了国内的强人际关系可能并不适用于同行评审，这个差别也是挺有意思的。</p><h3 id="图书馆"><a href="#图书馆" class="headerlink" title="图书馆"></a>图书馆</h3><p>这个问题单纯是吐槽类的了，图书馆的校外访问系统自从寒假放假就挂了，而且这个系统并不仅仅是用于校外访问，国外期刊的在线浏览是没有校园ip这个概念的，都是通过机构账号进行登录，而对于中国大学，采用的登陆系统都是各大学的“校外访问”的认证系统，所以这个系统挂了以后就没法进行这些期刊的登录了。</p><p>在线浏览的好处就是文章是单列分布的，pdf文档大部分都是双列分布，所以我更喜欢在线浏览，字体调节也更加方便。</p><p>更新：校外访问系统恢复了，但是没有给21级开账号！打电话给网络中心，答复是校外老师进不来，没法给21级学生录入系统。不过幸好之前学校的系统还没把毕业生的信息删除，暂时还能用。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;还没想好摘要&lt;/p&gt;</summary>
    
    
    
    <category term="周记" scheme="http://blog.176free.top/categories/%E5%91%A8%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Intro and intermediate Machine Learning 课程总结</title>
    <link href="http://blog.176free.top/2022/04/09/IntroAndIntermediateMachineLearningClassSummary/"/>
    <id>http://blog.176free.top/2022/04/09/IntroAndIntermediateMachineLearningClassSummary/</id>
    <published>2022-04-09T14:32:26.000Z</published>
    <updated>2023-12-10T08:45:02.466Z</updated>
    
    <content type="html"><![CDATA[<p>回顾一下Kaggle里两个简短的入门课程里用到的东西</p><span id="more"></span><h2 id="Intro-to-Machine-Learning"><a href="#Intro-to-Machine-Learning" class="headerlink" title="Intro to Machine Learning"></a>Intro to Machine Learning</h2><h3 id="基础探索："><a href="#基础探索：" class="headerlink" title="基础探索："></a><strong>基础探索：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">describe() <span class="comment">#描述每一列的计数、均值、标准差等数据</span></span><br><span class="line">head() <span class="comment">#列出前X行的数据</span></span><br></pre></td></tr></table></figure><h3 id="建立第一个模型："><a href="#建立第一个模型：" class="headerlink" title="建立第一个模型："></a><strong>建立第一个模型：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> Dmelbourne_model.predict(X.head()ecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define model. Specify a number for random_state to ensure same results each run</span></span><br><span class="line">melbourne_model = DecisionTreeRegressor(random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit model</span></span><br><span class="line">melbourne_model.fit(X, y) </span><br><span class="line"></span><br><span class="line"><span class="comment">#predict data</span></span><br><span class="line">melbourne_model.predict(X.head()</span><br></pre></td></tr></table></figure><p>这里用的是一个决策树模型，指定random_state的数字是为了在模型初始化时确定好模型内的参数，从而使得每次使用相同的random_state运行模型时都可以得到相同的结果。</p><p>通常model.fit()里的参数都是训练集，而model.predict()里的参数则是验证集或测试集。</p><h3 id="模型验证："><a href="#模型验证：" class="headerlink" title="模型验证："></a><strong>模型验证：</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line">predicted_home_prices = melbourne_model.predict(X)</span><br><span class="line">mean_absolute_error(y, predicted_home_prices)</span><br></pre></td></tr></table></figure><p>mean_absolute_error则为平均绝对值误差，即对所有项进行预测值与实际值求差后的平方和的均值。</p><p><strong>决策树VS随机森林：</strong></p><p>决策树单一，随机森林由多个决策树组成，且随机挑选特征</p><h2 id="Intermediate-Machine-Learning"><a href="#Intermediate-Machine-Learning" class="headerlink" title="Intermediate Machine Learning"></a>Intermediate Machine Learning</h2><h3 id="缺失值处理："><a href="#缺失值处理：" class="headerlink" title="缺失值处理："></a><strong>缺失值处理：</strong></h3><ol><li><p>舍弃存在缺失值的列</p></li><li><p>插值：如在数值列中将空缺数据的表格填上指定的数值。</p></li><li><p>高级插值：通过均值、最大值、最小值等数据进行综合决定</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#确定存在缺失值的列名</span></span><br><span class="line">cols_with_missing = [col <span class="keyword">for</span> col <span class="keyword">in</span> X.columns <span class="keyword">if</span> X[col].isnull().<span class="built_in">any</span>()]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#快速插值</span></span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imputation</span></span><br><span class="line">my_imputer = SimpleImputer()</span><br><span class="line">imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))</span><br><span class="line">imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))</span><br></pre></td></tr></table></figure><h3 id="类别变量："><a href="#类别变量：" class="headerlink" title="类别变量："></a><strong>类别变量：</strong></h3><ol><li><p>舍弃存在类别变量的列</p></li><li><p>序数编码</p><ol><li>把为数不多的类别型变量按顺序排列，根据权重或顺序等方式确定每个变量的数值</li></ol></li><li><p>One_Hot编码</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#确定哪一列是类别型变量：</span></span><br><span class="line">s = (X.dtypes == <span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">object_cols = <span class="built_in">list</span>(s[s].index)</span><br></pre></td></tr></table></figure><p>如果一个列中有文本，那么这一列的dtype就会是object</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make copy to avoid changing original data </span></span><br><span class="line">label_X_train = X_train.copy()</span><br><span class="line">label_X_valid = X_valid.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply ordinal encoder to each column with categorical data</span></span><br><span class="line">ordinal_encoder = OrdinalEncoder()</span><br><span class="line">label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])</span><br><span class="line">label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply one-hot encoder to each column with categorical data</span></span><br><span class="line">OH_encoder = OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>, sparse=<span class="literal">False</span>)</span><br><span class="line">OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))</span><br><span class="line">OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># One-hot encoding removed index; put it back</span></span><br><span class="line">OH_cols_train.index = X_train.index</span><br><span class="line">OH_cols_valid.index = X_valid.index</span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove categorical columns (will replace with one-hot encoding)</span></span><br><span class="line">num_X_train = X_train.drop(object_cols, axis=<span class="number">1</span>)</span><br><span class="line">num_X_valid = X_valid.drop(object_cols, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add one-hot encoded columns to numerical features</span></span><br><span class="line">OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=<span class="number">1</span>)</span><br><span class="line">OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="管道整合"><a href="#管道整合" class="headerlink" title="管道整合"></a>管道整合</h3><p>在对整体数据的特征把握清楚后，可以对数据处理的步骤进行整合</p><p><strong>1.管道的步骤</strong></p><blockquote><ul><li><p>对数值型数据列进行缺失值处理</p></li><li><p>对类别型数据列进行One—Hot编码</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing for numerical data</span></span><br><span class="line">numerical_transformer = SimpleImputer(strategy=<span class="string">&#x27;constant&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing for categorical data</span></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;most_frequent&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;onehot&#x27;</span>, OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bundle preprocessing for numerical and categorical data</span></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numerical_transformer, numerical_cols),</span><br><span class="line">        (<span class="string">&#x27;cat&#x27;</span>, categorical_transformer, categorical_cols)</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><p><strong>2.定义模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line">model = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p><strong>3.整合上述工作到管道中</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bundle preprocessing and modeling code in a pipeline</span></span><br><span class="line">my_pipeline = Pipeline(steps=[(<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),</span><br><span class="line">                              (<span class="string">&#x27;model&#x27;</span>, model)</span><br><span class="line">                             ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing of training data, fit model </span></span><br><span class="line">my_pipeline.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocessing of validation data, get predictions</span></span><br><span class="line">preds = my_pipeline.predict(X_valid)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">score = mean_absolute_error(y_valid, preds)</span><br></pre></td></tr></table></figure><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>由于采用交叉验证需要进行额外步骤，所以会对计算机产生额外负担，当数据量较大时尽量不采用交叉验证。并且当数据量较大时训练集和验证集的数据划分时已经有了较大的随机性</p><p><strong>验证步骤：</strong></p><ol><li><p>将验证集平均划分为不同的子集</p></li><li><p>选其中一个子集为训练集，其余子集为验证集</p></li><li><p>除第二步中的子集以外选择一个子集为训练集，其余子集（包含第二步）为验证集</p></li><li><p>重复第三步，直至所有子集均被作为训练集录入模型。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiply by -1 since sklearn calculates *negative* MAE</span></span><br><span class="line">scores = -<span class="number">1</span> * cross_val_score(my_pipeline, X, y,</span><br><span class="line">                              cv=<span class="number">5</span>,</span><br><span class="line">                              scoring=<span class="string">&#x27;neg_mean_absolute_error&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="“调参侠”XGBoost"><a href="#“调参侠”XGBoost" class="headerlink" title="“调参侠”XGBoost"></a>“调参侠”XGBoost</h3><p>XGBoost是一个开源软件库，它为 C++、Java、Python、 R和Julia提供了一个梯度提升框架，适用于Linux、Windows和 mac os。根据项目的描述，它的目的在于提供一个”可扩展、可移植和分布式梯度提升(GBM、GBRT、GBDT)库“。</p><p><strong>调用：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"></span><br><span class="line">my_model = XGBRegressor()</span><br><span class="line">my_model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p><strong>调参：</strong></p><p><code>n_estimators：</code> 指定经过建模周期的次数。它等于我们在集成中包含的模型数量。太小会欠拟合，太大会过拟合，默认在100-1000</p><p><code>early_stopping_rounds：</code> 指定验证分数在多少轮没有提升时停止训练</p><p><code>learning_rate：</code> 我们可以将每个模型的预测乘以一个小数（称为学习率），然后再将它们相加，而不是通过简单地将每个组件模型的预测相加来获得预测。一般来说，小的学习率和大的<code>n_estimators</code>会产生更准确的 XGBoost 模型，尽管它也会花费更长的时间来训练模型，因为它在整个循环中进行了更多的迭代。默认情况下，XGBoost 设置 learning_rate&#x3D;0.1</p><p><code>n_jobs:</code> 在考虑运行时间的较大数据集上，可以使用并行更快地构建模型。通常将参数 n_jobs 设置为机器上的内核数。在较小的数据集上，这无济于事。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;回顾一下Kaggle里两个简短的入门课程里用到的东西&lt;/p&gt;</summary>
    
    
    
    <category term="Kaggle" scheme="http://blog.176free.top/categories/Kaggle/"/>
    
    
  </entry>
  
</feed>
